\documentclass{llncs}

\usepackage{times}
\usepackage{url}
\def\UrlFont{\tt}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{color}

\usepackage{listings}
\lstset{aboveskip=1.0ex,
        belowskip=1.0ex,
        showstringspaces=false, % no special string space
        mathescape=true,
        flexiblecolumns=false,
        basewidth=0.55em,
        basicstyle=\small\ttfamily}

\lstset{literate={->}{{$\rightarrow{}\!\!\!$}}3
       }

\lstnewenvironment{curry}{\lstset{backgroundcolor=\color[rgb]{0.9,0.9,0.9}}}{}
\lstnewenvironment{haskell}{}{}
\newcommand{\listline}{\vrule width0pt depth1.75ex}

\newcommand{\code}[1]{\mbox{\small\texttt{#1}}}
\newcommand{\ccode}[1]{``\code{#1}''}
\newcommand{\bs}{\char92\xspace} % backslash
\newcommand{\us}{\char95\xspace} % underscore
\newcommand{\Choice}[2]{#1 \mathop{?} #2}
\newcommand{\ol}[1]{\overline{#1}}

% ------------------------------------------------------------------
% SYMBOLS & FORMULAS

\newcommand{\funset}{\ensuremath{_{\cal S}}}

\begin{document}
\pagestyle{plain}
\sloppy

\title{KiCS2: A New Compiler from Curry to Haskell}

\author{
Bernd Bra{\ss}el
\kern1em
Michael Hanus
\kern1em
Bj{\"o}rn Peem{\"o}ller
\kern1em
Fabian Reck
}
\institute{
Institut f\"ur Informatik, CAU Kiel, D-24098 Kiel, Germany \\
\email{\{bbr|mh|bjp|fre\}@informatik.uni-kiel.de}
}

\maketitle

\begin{abstract}
In this paper we present our first results of a new system
to compile functional logic programs of the source language Curry
into purely functional Haskell programs.
Our implementation is based on the idea to represent
nondeterministic result values of an expression in a single data structure.
This enables the application of various search strategies
to extract values from the search space.
We show by several benchmarks that our implementation
can compete with or outperforms other existing implementations of Curry.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:Introduction}

Functional logic languages integrate the most important
features of functional and logic languages
(see \cite{AntoyHanus10CACM,Hanus07ICLP} for recent surveys).
In particular, they combine higher-oder functions and demand-driven
evaluation from functional programming with logic programming features
like nondeterministic search and computing with partial information
(logic variables).
The combination of these features
has led to new design patterns \cite{AntoyHanus02FLOPS}
and better abstractions for application programming,
e.g., as shown for programming with databases
\cite{BrasselHanusMueller08PADL,Fischer05},
GUI programming \cite{Hanus00PADL},
web programming \cite{Hanus01PADL,Hanus06PPDP,HanusKoschnicke10PADL},
or string parsing \cite{CaballeroLopez99}.

The implementation of functional logic languages is challenging
since a reasonable implementation has to support the operational
features mentioned above.
One possible approach
is the design of new abstract machines appropriately supporting
these operational features and implementing them in some (typically,
imperative) language, like C \cite{Lux99FLOPS}
or Java \cite{AntoyHanusLiuTolmach05,HanusSadre99JFLP}.
Another approach is the reuse of already existing implementations
of some of these features by translating
functional logic programs into logic or functional languages.
For instance, if one compiles into Prolog, one can reuse
the existing backtracking implementation for nondeterministic
search and logic variables and unification for computing with partial
information. However, one has to implement demand-driven evaluation
and higher-order functions \cite{AntoyHanus00FROCOS}.
A disadvantage of this approach is the commitment to a fixed
search strategy (backtracking).

If one compiles into a non-strict functional language like Haskell,
one can reuse the implementation
of lazy evaluation and higher-order functions, but one has
to implement nondeterministic evaluations
\cite{BrasselFischerHanusReck11,BrasselHuch07}.
Although Haskell offers list comprehensions to model
backtracking \cite{Wadler85}, this cannot be exploited
due to the specific semantical requirements of the combination
of non-strict and nondeterministic operations
\cite{GonzalezEtAl99}. Thus, additional implementation efforts
are necessary like implementation of shared
nondeterministic computations \cite{FischerKiselyovShan09}.

Nevertheless, the translation of functional logic languages
into other high-level languages is attractive:
it limits the implementation efforts compared to an implementation
from scratch and one can exploit the existing implementation technologies
provided that the efforts to implement the missing features
are reasonable.

In this paper we describe an implementation that is based
on the latter principles. We present a method to compile
programs written in the functional logic language Curry \cite{Hanus06Curry}
into Haskell programs based on the ideas shown
in \cite{BrasselFischer08IFL}.
The difficulty of such an implementation is the fact
that nondeterministic results can occur in any place of a
computation. Thus, one cannot separate logic computations
by the use of list comprehensions \cite{Wadler85},
but the outcome of any operation could be potentially nondeterministic,
i.e., it might have more than one result value.
We solve this problem by an explicit representation of
nondeterministic values, i.e., we extend each data type by
another constructor to represent the choice between several values.
This idea is also the basis of the Curry implementation KiCS
\cite{BrasselHuch07,BrasselHuch09}.
However, KiCS is based on unsafe features of Haskell
that inhibit the use of optimizations provided by Haskell compilers
like GHC.\footnote{\url{http://www.haskell.org/ghc/}}
In contrast, our implementation, called KiCS2, is not based on unsafe features.
In addition, we also support more flexible search strategies
and new features to encapsulate nondeterministic computations
(which are not described in detail in this paper due to lack of space).

The general objective of our approach is the support
of flexible strategies to explore the search space
resulting from nondeterministic computations.
In contrast to Prolog-based implementations
that use backtracking and, therefore, are incomplete,
we also want to support complete strategies like breadth-first search,
iterative deepening or parallel search (in order to exploit
multi-core architectures). We achieve this goal by an
explicit representation of the search space as data
that can be traversed by various operations.
Moreover, purely deterministic computations
are implemented as purely functional programs so that
they are executed with almost the same efficiency
of their purely functional counterparts.

In the next section, we sketch the source language Curry
and introduce a normalized form of Curry programs that is the
basis of our translation scheme.
Section~\ref{sec:Compilation} presents the basic ideas
of this translation scheme.
Benchmarks of our initial implementation of this scheme
are presented in Section~\ref{sec:Benchmarks}.
Further features of our system are sketched in
Section~\ref{sec:FurtherAspects}
before we conclude in Section~\ref{sec:Conclusions}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Curry Programs}
\label{sec:Curry}

The syntax of the functional logic language Curry \cite{Hanus06Curry}
is close to Haskell \cite{PeytonJones03Haskell}.
In addition, Curry allows free (logic) 
variables in conditions and right-hand sides of defining rules.
In contrast to functional programming and similarly to logic programming,
operations can be defined by overlapping rules so that
they might yield more than one result on the same input.
Such operations are also called \emph{nondeterministic}.
For instance, Curry offers a \emph{choice} operation that is predefined by
the following rules:
%
\begin{curry}
  x ? _ = x
  _ ? y = y
\end{curry}
%
Thus, we can define a nondeterministic operation \code{aBool} by
\label{ex:aBool}
%
\begin{curry}
  aBool = True ? False
\end{curry}
%
so that the expression \ccode{aBool} has two values:
\code{True} and \code{False}.

If nondeterministic operations are used as arguments in other operations,
a semantical ambiguity might occur. Consider the operations
%
\begin{curry}
  xor True  False = True
  xor True  True  = False
  xor False x     = x

  xorSelf x = xor x x
\end{curry}
%
and the expression \ccode{xorSelf aBool}.
If we interpret this program as a term rewriting system,
we could have the reduction
\begin{lstlisting}
  xorSelf aBool  $\to~$  xor aBool aBool     $\to~$  xor True aBool 
                 $\to~$  xor True False      $\to~$  True
\end{lstlisting}
leading to the unintended result \code{True}.
Note that this result cannot be obtained if we use a strict strategy
where arguments are evaluated before the function calls.
In order to avoid such dependencies on the evaluation strategies
and exclude such unintended results,
Gonz\'alez-Moreno et al.\ \cite{GonzalezEtAl99} proposed
the rewriting logic CRWL as a logical
(execution- and strategy-independent) foundation for declarative
programming with non-strict and nondeterministic operations.  This
logic specifies the \emph{call-time choice} semantics \cite{Hussmann92}
\label{ctc-semantics}
where values of the arguments of an operation are determined before the
operation is evaluated. Note that this does not necessarily require
an eager evaluation of arguments.
Actually, \cite{AlbertHanusHuchOliverVidal05,LopezRodriguezSanchez07}
define lazy evaluation strategies for functional logic programs
with call-time choice semantics where actual arguments passed to
operations are shared. Hence, we can evaluate the expression above
lazily provided that all occurrences of \code{aBool}
are shared so that all of them reduce either to \code{True} or to \code{False}.
The requirements of this call-time choice semantics is the
reason why it is not simply possible to use list comprehensions
or non-determinism monads for a straightforward implementation
of functional logic programs in Haskell \cite{FischerKiselyovShan09}.

Due to these considerations, an implementation of Curry
has to support lazy evaluation where operations can have multiple results
and unevaluated arguments must be shared.
Since this is a complex task, if we try to implement it
directly on the level of source programs,
we perform some simplifications on programs before we generate the target code.

First of all, we assume that our programs are \emph{first-order}.
Higher-order programs can be transformed into first-order programs
by a technique called ``defunctionalization'' \cite{Reynolds72}
which is often used in Prolog-based implementations of
functional logic languages \cite{AntoyHanus00FROCOS}.
However, we will later use the higher-order features of Haskell
instead of defunctionalization.

\label{sec:no-logic-vars}
Furthermore, we assume that our programs \emph{do not contain logic variables}.
This assumption can be made since it has been shown
\cite{AntoyHanus06ICLP} that logic variables can be replaced
by nondeterministic ``generators,''
i.e., operations that evaluate to all possible values of the type of
the logic variable. For instance, a Boolean logic variable
can be replaced by the generator \code{aBool} defined above.

\begin{figure}[t]
\[
\begin{array}{@{}rcl@{~~~~~~~~}l}
  e & ::= & x & \mbox{$x$ is a variable}\\
    &  |  & c(e_1,\ldots,e_n) & \mbox{$c$ is an $n$-ary constructor symbol}\\
    &  |  & f(e_1,\ldots,e_n) & \mbox{$f$ is an $n$-ary function symbol}\\
    &  |  & \Choice{e_1}{e_2} & \mbox{choice} \\[1ex]
%    &  |  & \mbox{\code{let}}~\{x_1=e_1;\ldots;x_n = e_n\}~\code{in}~e
%                                            & \mbox{let binding}\\[1ex]
  D & ::= & f(x_1,\ldots,x_n) = e
                & \mbox{$n$-ary function $f$ with a single rule}\\
    &  |  & f(c(y_1,\ldots,y_m),x_2,\ldots,x_n) = e
                & \mbox{matching rule for $n$-ary function $f$}\\
    &     & & \mbox{$c$ is an $m$-ary constructor symbol}\\
  P & ::= & \ol{D_k}
\end{array}
\]
\caption{Uniform programs}
\label{fig:uniform}
\end{figure}
%
Finally, we assume that the pattern matching strategy
is explicitly encoded in individual matching functions.
In contrast to \cite{AlbertHanusHuchOliverVidal05},
where the pattern matching strategy is encoded in case expressions,
we assume that each case expression is transformed into
a new operation in order to avoid complications by translating
nested case expressions.
Thus, we assume that all programs are \emph{uniform}
according to the definition in Fig.~\ref{fig:uniform}.
There, %$\ol{o_n}$ denotes the object sequence $o_1,\ldots,o_n$,
the variables in the left-hand sides of each rule are pairwise
different, and
the constructors in the left-hand sides of the matching rules of each function
are pairwise different.
Uniform programs have a simple form of pattern matching:
either a function is defined by a single rule without pattern matching,
or it is defined by rules where only one constructor occurs in the same argument
of all rules.\footnote{For simplicity, we require in
Fig.~\ref{fig:uniform} that the matching argument is always the
first one, but one can also choose any other argument.}
For instance, the operation \code{xor} defined above can be transformed
into the following uniform program:
%
\begin{curry}
  xor True   x = xor' x
  xor False  x = x
  xor' False = True
  xor' True  = False
\end{curry}
%
In particular, there are no overlapping rules for functions
(except for the choice operation \ccode{?} which is considered as predefined).
Antoy \cite{Antoy01PPDP} showed that each functional logic program,
i.e., each constructor-based conditional term rewriting system,
can be translated into an equivalent unconditional term rewriting system
without overlapping rules but containing choices in the right-hand sides,
also called LOIS (limited overlapping inductively sequential) system.
Furthermore, Brassel \cite{Brassel11Thesis} showed the semantical
equivalence of narrowing computations in LOIS systems
and rewriting computations in uniform programs.
Due to these results, uniform programs are a reasonable intermediate
language for our translation into Haskell which will be presented
in the following sections.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Compilation to Haskell: The Basics}
\label{sec:Compilation}

\subsection{Representing Nondeterministic Computations}

As mentioned above, our implementation is based on the explicit
representation of nondeterministic results in a data structure.
This could be easily achieved by adding a constructor
to each data type to represent a choice between two values.
For instance, one can redefine the data type for Boolean values
as follows:
\begin{haskell}
  data Bool = False | True | Choice Bool Bool
\end{haskell}
Thus, we can implement the nondeterministic operation \code{aBool}
defined in Section~\ref{ex:aBool} as follows:
\begin{haskell}
  aBool = Choice True False
\end{haskell}
If operations can deliver nondeterministic values,
we have to extend the rules for operation defined by
pattern matching so that they do not fail on nondeterministic argument
values. Instead, they move the nondeterministic choice one level above,
i.e., a choice in some argument leads to a choice in any result
of this operation (this is also called a ``pull-tab'' step in
\cite{AlqaddoumiAntoyFischerReck10}). For instance,
the rules of the uniform operation \code{xor} shown above
are extended as follows:
%
\begin{curry}
  xor True           x = xor' x
  xor False          x = x
  xor (Choice x1 x2) x = Choice (xor x1 x) (xor x2 x)

  xor' False          = True
  xor' True           = False
  xor' (Choice x1 x2) = Choice (xor' x1) (xor' x2)
\end{curry}
%
The operation \code{xorSelf} is not defined by a pattern matching rule
and, thus, need not be changed.
If we evaluate the expression \ccode{xorSelf aBool}, we get the result
\begin{lstlisting}
  Choice (Choice False True) (Choice True False)
\end{lstlisting}
How can we interpret this result?
In principle, the choices represent different possible values.
Thus, if we want to show the different values of an expression
(which is usually the task of a top-level ``read-eval-print'' loop),
we enumerate all values contained in the choices.
These are \code{False}, \code{True}, \code{True}, and \code{False}
in the result above.
Unfortunately, this is not conform with the call-time choice semantics
discussed in Section~\ref{ctc-semantics} which should not yield
a value like \code{True}.
The call-time choice semantics requires that the choice
of a value made for
the initial expression \code{aBool} should be consistent
in the entire computation.
For instance, if we select the value \code{False} for the
expression \code{aBool}, this selection should be made
at all other places where this expression might have been copied
during the computation. However, our initial implementation
duplicates the initially single \code{Choice} into finally three
occurrences of \code{Choice}.

We can correct this unintended behavior of our implementation
by identifying different \code{Choice} occurrences that are duplicates
of some single \code{Choice}. This can be easily done by attaching
a unique identifier, e.g., a number, to each choice:
\begin{haskell}
  type ID = Integer
  data Bool = False | True | Choice ID Bool Bool
\end{haskell}
Furthermore, we modify the \code{Choice} pattern rules so that
the identifiers will be kept, e.g.,
\begin{curry}
  xor (Choice i x1 x2) x = Choice i (xor x1 x) (xor x2 x)
\end{curry}
If the expression \code{aBool} assigns the number \code{1}
to its choice, we obtain the result
\begin{lstlisting}
  Choice 1 (Choice 1 False True) (Choice 1 True False)
\end{lstlisting}
when evaluating the expression \ccode{xorSelf aBool}.
If we show the values contained in this result,
we have to make \emph{consistent} selections in choices with
same identifiers. This, if we select the left branch as the value
of the outermost \code{Choice}, we also have to select the left branch
in the selected argument \code{(Choice 1 False True)} so that only
the value \code{False} is possible here.
Similarly, if we select the right branch as the value of the outermost
\code{Choice}, we also have to select the right branch in
its selected argument \code{(Choice 1 True False)} which yields to only
value \code{False}.

Note that each \code{Choice} occurring for the first time in a computation
has to get its own unique identifier.
For instance, if we evaluate the expression \ccode{xor aBool aBool},
the two occurrences of \code{aBool} assign different identifiers
to their \code{Choice} constructor (e.g., \code{1} for the left
and \code{2} for the right \code{aBool} argument) so that this evaluates to
\begin{lstlisting}
  Choice 1 (Choice 2 False True) (Choice 2 True False)
\end{lstlisting}
Here we can make different selections for the outermost and inner
\code{Choice} constructors so that this nondeterministic result represent
four values.

To summarize, our implementation is based on the following principles:
\begin{enumerate}
\item Each nondeterministic choice is represented by a \code{Choice}
constructor with a unique identifier.
\item When matching a \code{Choice} constructor, the choice is
moved to the result of this operation with the same identifier,
i.e., a nondeterministic argument
yields nondeterministic results for each of the argument's values.
\item Each choice occurring in a computation gets its own unique
identifier.
\end{enumerate}
The latter principle requires the creation of fresh identifiers
during a computation---a non-trivial problem in functional languages.
One possibility is the use of a global counter
that is accessed by unsafe features whenever a new identifier
is required. Unfortunately, unsafe features inhibit the use
of optimization techniques for pure functional programming
and makes the application of advanced evaluation and search
strategies (e.g., parallel strategies) more complex.
Therefore, we want to avoid unsafe features in our implementation
so that we have to thread some sort of global information
through our program in order to supply fresh references
at any point of a computation.
For this purpose, we assume a type \code{IDSupply}
with operations
\begin{haskell}
  thisID      :: IDSupply -> ID
  leftSupply  :: IDSupply -> IDSupply
  rightSupply :: IDSupply -> IDSupply
\end{haskell}
and add a new argument of type \code{IDSupply} to each operation
of the source program.

\code{thisID} creates a fresh identifier from a current
identifier supply, and
\code{leftSupply} and \code{rightSupply} create
new different identifier supplies (without the one obtained
by \code{thisID}) from a given one.
The latter are used when an operation calls two\footnote{The
extension to more than two is straightforward.}
other operations in the right-hand side of a rule. In this case,
the other operations must be supplied with their individual distinct
identifier supplies. For instance, the operation \code{main}
defined by
\label{sec:xor-main}
\begin{curry}
  main = xorSelf aBool
\end{curry}
is translated into
\begin{haskell}
  main s = xorSelf (aBool (leftSupply s)) (rightSupply s)
\end{haskell}
Any choice in the right-hand side of a rule gets its own identifier
by the operation \code{thisID}, as in
\begin{haskell}
  aBool s = Choice (thisID s) True False
\end{haskell}
There are various implementations of \code{IDSupply}.
The simplest implementation uses unbounded integers:
\begin{haskell}
  type IDSupply = Integer
  leftSupply  n = 2*n
  rightSupply n = 2*n+1
  thisID      n = n
\end{haskell}
There are other more sophisticated implementations available
\cite{AugustssonRittriSynek94}.
Actually, our compilation system is parameterized over different
implementations of \code{IDSupply}
in order to perform some experiments and choose
the most appropiate for a given application.
Each implementation must ensure that, if $s$ is a value of type
\code{IDSupply}, then \code{thisID($o_1$($\ldots$($o_n$ $s$)$\ldots$))}
and \code{thisID($o'_1$($\ldots$($o'_m$ $s$)$\ldots$))}
are different identifiers provided that
$o_i,o'_j \in \{\code{leftSupply},\code{rightSupply}\}$
and $o_1\cdots o_n \neq o'_1\cdots o'_m$.

\subsection{The Basic Translation Scheme}

Functional logic computations can also fail, e.g.,
due to partially defined operations.
Computing with failures is a typical programming technique
and provides for specific programming patterns \cite{AntoyHanus02FLOPS}.
Hence, in contrast to functional programming,
a failing computation should not abort the complete evaluation
but it should be considered as some part of a computation that
does not produce a meaningful result.
In order to implement this behavior, we extend
each datatype by a further constructor \code{Fail}
and complete each function containing matching rules
by a final rule that matches everything and returns the value \code{Fail}.
For instance, consider the definition of lists
\begin{curry}
  data List a = Nil | Cons a (List a)
\end{curry}
and an operation to extract the first element of a non-empty list:
\begin{curry}
  head :: List a -> a
  head (Cons x xs) = x
\end{curry}
The type definition is extended as follows:\footnote{Actually,
our compiler performs some renamings to avoid conflicts with
predefined Haskell entities and introduces type classes
to resolve overloaded symbols like \code{Choice} and \code{Fail}.}
\begin{haskell}
  data List a = Nil | Cons a (List a) | Choice (List a) (List a) | Fail
\end{haskell}
The operation \code{hread} is extended by an identifier supply
and further matching rules:
\begin{haskell}
  head :: List a -> IDSupply -> a
  head (Cons x xs)      s = x
  head (Choice i x1 x2) s = Choice i (head x1 s) (head x2 s)
  head _                s = Fail
\end{haskell}
Note that the final rule returns \code{Fail} if \code{head} is
applied to the empty list as well as if the matching argument
is already a failed computation, i.e., it also propagates failures.

As already discussed above, an occurrence of \ccode{?}
in the right-hand side is translated into a \code{Choice} supplied
with a fresh identifier by the operation \code{thisID}.
In order to ensure that each occurrence of \ccode{?} in the source program
get its own identifier, all choices and all operations in the right-hand
side of a rule get their own identier supplies via appropriate
applications of \code{leftSupply} and \code{rightSupply}
to the supply of the defined operation.
For instance, a rule like
\begin{curry}
  main2 = xor aBool (False ? True)
\end{curry}
is translated into
\begin{haskell}
  main2 s = let s1 = leftSupply  s
                s2 = rightSupply s
                s3 = leftSupply  s2
                s4 = rightSupply s2
             in xor (aBool s3) (Choice (thisID s4) False True) s1
\end{haskell}
An obvious optimization, performed by our compiler,
is a \emph{determinism analysis}.
If an operation does not call, directly or indirectly through
other operations, the choice operation \ccode{?},
then it is not necessary to pass a supply for identifiers.
In this case, the \code{IDSupply} argument can be omitted
so that the generated code is nearly identical to a corresponding
functional program (apart from the additional rules to match
the constructors \code{Choice} and \code{Fail}).

As mentioned in Section~\ref{sec:no-logic-vars},
our compiler translates occurrences of logic variables
into generators. Since these generators are standard
nondeterministic operations, they are translated like any other operation.
For instance, the operation \code{aBool} is a generator for
Boolean values and its translation into Haskell has been presented above.

A more detailed discussion of this translation scheme can be found
in the original proposal \cite{BrasselFischer08IFL}.
The correctness of this transformation from nondeterministic source programs
into deterministic target programs has been formally shown
in \cite{Brassel11Thesis}.


\subsection{Extracting Values}

So far, our generated operations compute all the nondeterministic
values of an expression represented by a structure containing
\code{Choice} constructors. In order to extract the various
values from this structure, we have to define operations
that compute all possible choices in some order where the choice identifiers
are taken into account.
To provide a common interface for such operations, we introduce
a data type to represent the general outcome of a computation,
\begin{haskell}
  data Try a = Val a | Choice ID a a | Fail
\end{haskell}
together with an auxiliary operation:\footnote{Note that the
operation \code{try} is not really polymorphic but overloaded
for each data type and, therefore, defined in instances of some type class.}
\begin{haskell}
  try :: a -> Try a
  try (Choice i x y) = Choice i x y
  try Fail           = Fail
  try x              = Val x
\end{haskell}
In order to take the identity of choices into account when extracting values,
one has to remember which choice (e.g., left or right branch)
has been made for some particular choice.
Therefore, we introduce the type
\begin{haskell}
  data Choice = NoChoice | ChooseLeft | ChooseRight
\end{haskell}
where \code{NoChoice} represents the fact that a choice has not yet been made.
Furthermore, we need operations to lookup the current choice
for a given identifier or change its choice:
\begin{haskell}
  lookupChoice :: ID -> IO Choice
  setChoice    :: ID -> Choice -> IO ()
\end{haskell}
In Haskell, there are different possibilities to implement a mapping
from choice identifiers to some value of type \code{Choice}.
Our implementation support various options together with various
implementations of \code{IDSupply}.
For instance, a simple implementation can be obtained by
using updatable values, i.e., the Haskell type \code{IORef}.
In this case, choice identifiers are just memory cells in Haskell:\footnote{%
In this case, integers cannot be used for \code{IDSupply}.
Nevertheless,
this type can be implemented as an infinite tree containing
all memory cells required in a computation that are created
on demand with initial value \code{NoChoice}.}
\begin{haskell}
  newtype ID = ID (IORef Choice)
\end{haskell}
The implementation of the lookup and set operations is straightforward:
\begin{haskell}
  lookupChoice (ID ref) = readIORef ref
  setChoice (ID ref) c = writeIORef ref c
\end{haskell}
Now we can print all values contained in a choice structure
in a depth-first manner by the following operation:
\label{sec:printValsDFS}
\begin{haskell}
  printValsDFS :: Try a -> IO ()
  printValsDFS (Val v)          = print v
  printValsDFS Fail             = return ()
  printValsDFS (Choice i x1 x2) = lookupChoice i >>= choose
   where
    choose ChooseLeft  = printValsDFS (try x1)
    choose ChooseRight = printValsDFS (try x2)
    choose NoChoice    = do newChoice ChooseLeft  x1
                            newChoice ChooseRight x2
  
    newChoice ch x = do setChoice i ch
                        printValsDFS (try x)
                        setChoice i NoChoice
\end{haskell}
This operation prints a computed value and ignores failures.
If there is some choice, it checks whether a choice for
this identifier has already been made (note that the initial value
for all identifiers is \code{NoChoice}).
If a choice has been made, it follows this choice.
Otherwise, the left choice is made and stored. After printing
all the values w.r.t.\ this choice, the choice is undone (like in backtracking)
and the right choice is made and stored.

For instance, to print all values of the expression \code{main}
defined in Section~\ref{sec:xor-main},
we evaluate the Haskell expression \ccode{printValsDFS (try (main inits))}
where \code{inits} is the initial value for \code{IDSupply}
(where all references are initialized with \code{NoChoice}).
Thus, we obtain the output
\begin{haskell}
  False
  False
\end{haskell}
Of course, printing all values via depth-first search
is only one option which is not sufficient in case of infinite search
spaces. For instance, one can easily define an operation
that prints only the first solution. Due to the lazy evaluation
strategy of Haskell, such an operation can also be applied to
infinite choice structures.
In order to abstract from these different printing options,
our implementation contains a more general
approach by translating choice structures into monadic structures
w.r.t.\ various strategies (depth-first search, breadth-first search,
iterative deepening, parallel search).
This allows an independent processing of the resulting monadic structures,
e.g.,
by an interactive loop where the user can request the individual values.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Benchmarks}
\label{sec:Benchmarks}

In this section we evaluate our compiler by comparing the
efficiency of the generated Haskell programs to various
other systems, in particular, other implementations of Curry.
For our comparison with other Curry implementations,
we consider PAKCS \cite{Hanus10PAKCS} (Version 1.9.2) which compiles
Curry into Prolog \cite{AntoyHanus00FROCOS}
(based on SICStus-Prolog 4.1.2, but a SWI-Prolog 5.10 back end is also
available
but much slower). PAKCS has been used for a number of practical
applications of Curry.
Another mature implementation which we consider is MCC \cite{Lux99FLOPS}
(Version 0.9.10) which compiles Curry into C.

The functional logic language Toy \cite{Lopez-FraguasSanchez-Hernandez99}
has many similarities to Curry and the Toy system compiles Toy programs
into Prolog programs. However, we have not included a comparison
in this paper since \cite{AntoyHanus00FROCOS} contains benchmarks
showing that the implementation of sharing used in PAKCS produces
more efficient programs.

Our compiler has been executed with the Glasgow Haskell Compiler
(GHC 6.12.1, option -O2). All benchmarks were executed on a Linux machine
(Ubuntu 10.10) with an Intel Core i5 (2.53GHz) processor.
The timings were performed with the time command measuring the
execution time (in seconds) of a compiled executable for each benchmark.

\begin{figure}
\centering
\begin{tabular}{|@{~~}l@{~~}|*{4}{@{~~}r@{~~}|}}
\hline
System  & ReverseUser & Reverse &  Tak  & TakPeano \\\hline
KiCS2   &        0.13 &    0.11 &  0.24 &     0.82 \\
PAKCS   &        2.07 &    1.91 & 39.19 &    60.69 \\
MCC     &        0.37 &    0.38 &  1.03 &     3.99 \\
GHC     &        0.11 &    0.10 &  0.04 &     0.50 \\
SICStus &        0.39 &    0.28 &  0.50 &     4.82 \\
SWI     &        1.33 &    1.14 &  1.87 &    12.72 \\
\hline
\end{tabular}
\caption{Benchmarks: first-order functional programs}
 \label{fig:bench-first-order}
\end{figure}
%
The first collection of benchmarks (Fig.~\ref{fig:bench-first-order})
are purely first-order functional programs.
The Prolog (SICStus, SWI) and Haskell (GHC) programs have been rewritten
according to the Curry formulation.
``ReverseUser'' is the naive reverse program applied to a list of 4096 elements,
where all data (lists, numbers) are user-defined.
``Reverse'' is the same but with built-in lists.
``Tak'' is a highly recursive function on naturals applied
to arguments (27,16,8) and ``TakPeano'' is the same but with
user-defined natural numbers in Peano representation.
Note that the Prolog programs use a strict evaluation strategy
in contrast to all others. Thus, the difference between PAKCS and SICStus
shows the overhead to implement lazy evaluation in Prolog.

One can deduce from these results that one of the initial
goals for this compiler is satisfied, since functional Curry programs
are executed almost with the same speed as their Haskell equivalents.
An overhead is visible if one uses built-in numbers
(due to the potential nondeterministic values, KiCS2 cannot
directly map operations on numbers into the Haskell primitives)
where GHC can apply specific optimizations.

\begin{figure}
\centering
\begin{tabular}{|@{~~}l@{~~}|*{5}{@{~~}r@{~~}|}}
\hline
System  & ReverseHO & Primes & PrimesPeano & Queens & QueensUser \\\hline
KiCS2   &     32.82 &   1.12 &        0.29 &   9.77 &     12.41  \\
KiCS2HO &      0.21 &   0.09 &        0.27 &   0.63 &      0.72  \\
PAKCS   &      7.24 &  13.55 &       22.74 &  78.52 &     81.98  \\
MCC     &      0.22 &   0.32 &        1.26 &   2.65 &      2.93  \\
GHC     &      0.21 &   0.06 &        0.21 &   0.05 &      0.12  \\
\hline
\end{tabular}
\caption{Benchmarks: higher-order functional programs}
 \label{fig:bench-higher-order}
\end{figure}
%
The next collection of benchmarks (Fig.~\ref{fig:bench-higher-order})
considers higher-order functional programs so that we drop
the comparison to first-order Prolog systems.
``ReverseHO'' reverses a list with one million
elements in linear time using higher-order functions
like \code{foldl} and \code{flip}.
``Primes'' computes the the 2000.\ prime number via the
sieve of Eratosthenes using higher-order functions,
and ``PrimesPeano'' computes the 256.\ prime number but
with Peano numbers and user-defined lists.
Finally, ``Queens'' (and ``QueensUser'' with user-defined lists)
computes the number of safe positions of queens on a $11 \times 11$
chess board.

As discussed above, our compiler performs an optimization
when all operations are deterministic. However, in the case
of higher-order functions, this optimization cannot be performed
since any operation, i.e., also a nondeterministic operation,
can be passed as an argument. As shown in the first line of this table,
this considerably reduces the overall performance.
To improve this situation, our compiler generates two versions
of a higher-order function: a general version applicable to any
argument and a specialized version where all higher-order arguments
are assumed to be deterministic operations.
Moreover, we implemented a program analysis
to approximate those operations that call higher-order functions
with deterministic operations so that one can use their
specialized versions. The result of this improvement
is shown as ``KiCS2HO'' and demonstrates its usefulness.
Therefore, it is always used in the subsequent benchmarks.

\begin{figure}
\centering
\begin{tabular}{|@{~~}l@{~~}|*{4}{@{~~}r@{~~}|}}
\hline
System  & PermSort & PermSortPeano &  Last & RegExp \\\hline
KiCS2   &     2.61 &         3.99  &  0.18 &   0.50 \\
PAKCS   &    26.08 &        67.51  &  2.58 &  12.46 \\
MCC     &     1.23 &         4.00  &  0.08 &   0.46 \\
\hline
\end{tabular}
\caption{Benchmarks: nondeterministic functional logic programs}
 \label{fig:bench-nondet}
\end{figure}
%
To evaluate the efficiency of nondeterministic computations
(Fig.~\ref{fig:bench-nondet}),
we sorted a list containing 15 elements by enumerating all permutations
and selecting the sorted ones (``PermSort'' and ``PermSortPeano''
for Peano numbers),
computed the last element \code{x}
of a list \code{xs} containing 100,000 elements
by solving the equation \ccode{ys++[x] =:= xs}
(the implementation of unification and variable bindings
require some additional machinery that is sketched in
Section~\ref{sec:unification}),
and matched a regular expression in a string of length 200,000
following the nondeterministic specification of \code{grep} shown
in \cite{AntoyHanus10CACM}.
Although the results show that our high-level implementation is not far
from the efficiency of MCC,
and it is superior to PAKCS which exploits Prolog features
like backtracking and logic variables for these benchmarks.

Since our implementation represents nondeterministic values
as Haskell data structures, we get, in contrast to most
other implementations of Curry, one interesting improvement for free:
deterministic subcomputations are shared even if they occur
in different nondeterministic computations.
To show this effect of our implementation, consider the
nondeterministic sort operation \code{psort} (permutation sort)
and the infinite list of all prime numbers \code{primes},
as used in the previous benchmarks, and the following definitions:
\begin{curry}
goal1 = [primes!!1003, primes!!1002, primes!!1001, primes!!1000]
goal2 = psort [7949,7937,7933,7927]
goal3 = psort [primes!!1003, primes!!1002, primes!!1001, primes!!1000]
\end{curry}
In principle, one would expect that the sum of the execution times
of \code{goal1} and \code{goal2} is equal to the time to execute
\code{goal3}. However, implementations based on backtracking
evaluate the primes occurring in \code{goal3} multiple times,
as can be seen by the run times for PAKCS and MCC shown
in Fig~\ref{fig:bench-sharing-over-nondet}.
%
\begin{figure}
\centering
\begin{tabular}{|@{~~}l@{~~}|*{3}{@{~~}r@{~~}|}}
\hline
System  & \code{goal1} & \code{goal2} & \code{goal3} \\\hline
KiCS2   &        0.46  &        0.00  &        0.46  \\
PAKCS   &       13.70  &        0.02  &      139.55  \\
MCC     &        0.30  &        0.00  &        3.06  \\
\hline
\end{tabular}
\caption{Benchmarks: sharing over nondeterminism}
 \label{fig:bench-sharing-over-nondet}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Further Features}
\label{sec:FurtherAspects}

In this section we sketch some additional features of our implementation.
Due to lack of space, we cannot discuss all details of them.

\subsection{Search Strategies}

Due to the fact that we represent nondeterministic results
in a data structure rather as a computation as in implementations
based on backtracking, we can provide different methods to
explore the search space containing the different result values.
We have already seen in Section~\ref{sec:printValsDFS}
how we can explore this search space to print all values in
depth-first order.
Apart from this simple approach,
our implementation contains various strategies
(depth-first, breadth-first, iterative deepening, parallel search)
to transform a choice structure into a list of results
that can be printed in different ways (e.g., all solutions, only the first
solutions, or one after another by user requests).
Actually, the user can set options to select the search strategy and
the printing method.

\begin{figure}
\centering
\begin{tabular}{|@{~~}l@{~~}|*{3}{@{~~}r@{~~}|}}
\hline
Method               & PermSort & PermSortPeano &  NDNums \\\hline
\code{printValsDFS}  &    2.61  &         3.99  & $\infty$ \\
depth-first search   &    5.10  &         7.62  & $\infty$ \\
breadth-first search &   18.54  &        20.23  &    10.96 \\
iterative deepening  &    8.33  &        10.01  &     3.10 \\
%parallel (1 proc.)   &    5.84  &         6.54  & $\infty$ \\
%parallel (2 proc.)   &    3.46  &         3.79  & $\infty$ \\
\hline
\end{tabular}
\caption{Benchmarks: comparing different search strategies}
 \label{fig:search-strategies}
\end{figure}
%
In order to compare the various search strategies,
Fig.~\ref{fig:search-strategies} contains some corresponding
benchmarks.
``PermSort'' and ``PermSortPeano'' are the programs
discussed in Fig.~\ref{fig:bench-nondet} and ``NDNums'' is the program
\begin{curry}
  f n = f (n+1) ? n
\end{curry}
where we look for the first solution of \ccode{f 0 == 25000}
(obviously, depth-first search strategies do not terminate on this
expression).
All strategies except for the ``direct print'' method \code{printValsDFS}
translate choice structures into monadic list structures
in order to print them according to the user options.
The benchmarks show that the overhead of this transformation
is acceptable so that this more flexible approach is the default one.

We also made initial benchmarks with the parallel strategy
where nondeterministic choices are explored via GHC's \code{par}
construct. For the permutation sort we obtained a speedup of 1.7
when executing the same program on two processors, but no essential
speedup is obtained for more than two processors. 
Better results require a careful analysis of the synchronization
caused by the global structure to manage the state of choices.
This is a topic for future work.

\subsection{Encapsulated Search}

Instead of choosing different search strategies to evaluate
top-level expressions, our system also contains a primitive operation
\begin{curry}
 searchTree :: a -> SearchTree a
\end{curry}
to translate the search space caused by the evaluation of its argument
into a tree structure of the form
\begin{curry}
 data SearchTree a = Value a | Fail | Or (SearchTree a) (SearchTree a)
\end{curry}
With this primitive,
the programmer can define its own search strategy or to
collect all nondeterministic values into a list structure
for further processing \cite{BrasselHuch07}.

% TODO: set functions??

\subsection{Logic Variables and Unification}
\label{sec:unification}

Although our implementation is based on eliminating all logic
variables from the source program by introducing generators,
many functional logic programs contain equational constraints
(e.g., compare the example ``Last'' of Fig.~\ref{fig:bench-nondet})
to put conditions on computed results.
Solving such conditions by generating all values is not always
a reasonable approach. For instance, if \code{xs} and \code{ys}
are free variables of type \code{[Bool]}, the equational
constraint \ccode{xs=:=ys} has an infinite number of solutions.
Instead of enumerating all these solutions, it is preferable to
delay this enumeration but remember the condition that both
\code{xs} and \code{ys} must always be evaluated to the same value.
This demands for extending the representation of nondeterministic
values by the possibility to add equational constraints between
different choice identifiers.
Due to lack of space, we have to omit the detailed description
of this extension.
However, it should be noted that the examples
``Last'' and ``RegExp'' of Fig.~\ref{fig:bench-nondet}
shows that unification can be supported with a reasonable efficiency.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Related Work}
\label{sec:Conclusions}

We have presented a new system to compile functional logic programs
into purely functional programs.
In order to be consistent with the call-time choice semantics
of functional logic languages like Curry or Toy,
we represent nondeterministic values
in choice structures where each choice has an identification.
Values for such choice identifiers are passed through
nondeterministic operations so that fresh identifiers are available
when a new choice needs to be created.
The theoretical justification of this implementation technique
is provided in \cite{Brassel11Thesis}.
Apart from the parser, where we reused an existing one
implemented in Haskell, the compiler is completely written in Curry.

Due to the representation of nondeterministic values as data,
our system easily supports various search strategies
in constrast to Prolog-based implementations.
Since we compile Curry programs into Haskell,
we can exploit the implementation efforts done
for functional programming.
Hence, purely functional parts of functional logic programs
can be executed with almost the same efficiency as Haskell programs.
Our benchmarks show that even the execution of the
nondeterministic parts can compete with other
implementations of Curry.
Furthermore, we can also exploit advanced developments
in the implementation of Haskell, like the parallel evaluation of
expressions, which is an interesting topic for future work.
Finally, our implementation has many opportunities
for optimization, like
better program analyses to approximate purely deterministic computations.

\bibliographystyle{plain}
\bibliography{mh}

\end{document}

% LocalWords:  Curry

